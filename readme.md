# ü§ñ Agente Conversacional Acad√©mico con LangChain# En el archivo: README.md



[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)# Agente Conversacional para Asistencia Acad√©mica

[![LangChain](https://img.shields.io/badge/LangChain-0.2.5-green.svg)](https://www.langchain.com/)

[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)Este proyecto es un agente conversacional inteligente desarrollado para la materia de Inteligencia Artificial I. Su objetivo es proporcionar asistencia 24/7 a los estudiantes, respondiendo preguntas sobre reglamentos, calculando promedios y consultando FAQs.



> **Sistema inteligente de asistencia acad√©mica 24/7** desarrollado con LangChain y m√∫ltiples proveedores LLM. El agente puede calcular promedios, buscar informaci√≥n en FAQs y consultar reglamentos acad√©micos usando b√∫squeda sem√°ntica.## Caracter√≠sticas



---*   **Orquestaci√≥n de Herramientas**: El agente puede decidir qu√© herramienta usar para cada tarea espec√≠fica.

*   **Calculadora Acad√©mica**: Calcula promedios de notas a partir del lenguaje natural.

## Tabla de Contenidos*   **Buscador de FAQ**: Responde preguntas frecuentes almacenadas en una base de conocimientos.

*   **Buscador Sem√°ntico Local**: Busca en documentos de texto (como reglamentos) para encontrar respuestas precisas.

- [Caracter√≠sticas](#-caracter√≠sticas)*   **Filtro de Seguridad**: Bloquea consultas inapropiadas para un comportamiento √©tico y seguro.

- [Arquitectura del Sistema](#-arquitectura-del-sistema)

- [Requisitos Previos](#-requisitos-previos)## Tecnolog√≠as Utilizadas

- [Instalaci√≥n](#-instalaci√≥n)

- [Configuraci√≥n](#-configuraci√≥n)*   **Python 3.10+**

- [Uso](#-uso)*   **LangChain**: Framework principal para la orquestaci√≥n del agente.

- [Estructura del Proyecto](#-estructura-del-proyecto)*   **Hugging Face**: Para el acceso a modelos de lenguaje de c√≥digo abierto (Mistral-7B).

- [Herramientas Disponibles](#-herramientas-disponibles)*   **Sentence-Transformers**: Para la creaci√≥n de embeddings vectoriales.

- [Selector de Proveedores LLM](#-selector-de-proveedores-llm)*   **FAISS (Facebook AI Similarity Search)**: Para la base de datos vectorial y b√∫squeda sem√°ntica.

- [Notebooks de Demostraci√≥n](#-notebooks-de-demostraci√≥n)

- [Troubleshooting](#-troubleshooting)## Instalaci√≥n y Configuraci√≥n

- [Contribuir](#-contribuir)

Sigue estos pasos para ejecutar el proyecto en tu m√°quina local.

---

### 1. Clonar el Repositorio

## ‚ú® Caracter√≠sticas```bash

git clone https://github.com/TitaniaWagic/Agente-conversacional-para-asistencia-academica-con-herramientas.git

### Funcionalidades Principalescd proyecto-agente-academico

- **Calculadora Acad√©mica**: Extrae y calcula promedios de notas desde lenguaje natural
  - Ejemplo: *"¬øCu√°l es el promedio de 8, 9 y 7?"* ‚Üí `8.0`
  
- **B√∫squeda en FAQ**: Consulta r√°pida de preguntas frecuentes
  - Base de conocimientos en JSON con respuestas predefinidas
  - B√∫squeda por similitud de texto
  
- **B√∫squeda Sem√°ntica en Reglamentos**: RAG (Retrieval-Augmented Generation)
  - Embeddings vectoriales con `sentence-transformers`
  - Base de datos vectorial FAISS para b√∫squeda eficiente
  - Responde preguntas sobre reglamentos acad√©micos complejos

- **Filtro de Seguridad**: Bloquea consultas inapropiadas
  - Previene consultas sobre respuestas de ex√°menes
  - Control de comportamiento √©tico del agente

- **Selector Multi-LLM**: Cambia f√°cilmente entre proveedores
  - **Google Gemini** (gemini-2.5-flash) - Por defecto
  - **Hugging Face** (Mistral-7B-Instruct-v0.2)
  - Extensible a otros proveedores (OpenAI, Anthropic, etc.)

### Patr√≥n de Agente ReAct

El agente utiliza el patr√≥n **ReAct** (Reasoning + Acting):
1. **Question**: Recibe la pregunta del usuario
2. **Thought**: Analiza qu√© herramienta usar
3. **Action**: Selecciona la herramienta apropiada
4. **Action Input**: Proporciona los par√°metros
5. **Observation**: Recibe el resultado de la herramienta
6. **Final Answer**: Genera la respuesta final

---

## üèóÔ∏è Arquitectura del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Usuario Final                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Filtro de Seguridad                         ‚îÇ
‚îÇ          (Bloquea consultas inapropiadas)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Agente ReAct (LangChain)                        ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ   Selector de LLM                           ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Google Gemini (por defecto)             ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Hugging Face (Mistral-7B)               ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ   Prompt ReAct Mejorado                     ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Instrucciones estrictas                 ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Ejemplos de uso                         ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Control de iteraciones (max 5)          ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 ‚ñº              ‚ñº              ‚ñº              ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇCalculadora‚îÇ  ‚îÇBuscador  ‚îÇ  ‚îÇ  Buscador  ‚îÇ  ‚îÇ Futura   ‚îÇ
         ‚îÇ Acad√©mica ‚îÇ  ‚îÇ   FAQ    ‚îÇ  ‚îÇReglamentos ‚îÇ  ‚îÇ  Tool    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ             ‚îÇ               ‚îÇ
               ‚îÇ             ‚îÇ               ‚ñº
               ‚îÇ             ‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
               ‚îÇ             ‚îÇ        ‚îÇVector Store  ‚îÇ
               ‚îÇ             ‚îÇ        ‚îÇ   (FAISS)    ‚îÇ
               ‚îÇ             ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ             ‚îÇ
               ‚ñº             ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ      Respuesta Final            ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Requisitos Previos

### Software Requerido

- **Python**: 3.10 o superior
- **pip**: Gestor de paquetes de Python
- **git**: Para clonar el repositorio (opcional)

### APIs Necesarias

Para usar el selector de LLM, necesitar√°s al menos una de estas claves:

1. **Google Gemini API** (Recomendado - Gratis)
   - Registro: [Google AI Studio](https://makersuite.google.com/app/apikey)
   - Modelo usado: `gemini-2.5-flash`

2. **Hugging Face API** (Alternativa)
   - Registro: [Hugging Face](https://huggingface.co/settings/tokens)
   - Modelo usado: `mistralai/Mistral-7B-Instruct-v0.2`

---

## Instalaci√≥n

### 1. Clonar el Repositorio

```bash
git clone https://github.com/TitaniaWagic/Agente-conversacional-para-asistencia-academica-con-herramientas.git
cd "TP final IA"
```

### 2. Crear Entorno Virtual (Recomendado)

**Windows (PowerShell):**
```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```

**Linux/Mac:**
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Instalar Dependencias

```bash
pip install -r requirements.txt
```

**Dependencias principales instaladas:**
- `langchain==0.2.5` - Framework principal
- `langchain-google-genai` - Cliente de Google Gemini
- `langchain-huggingface==0.0.3` - Cliente de Hugging Face
- `sentence-transformers>=2.7.0` - Para embeddings
- `faiss-cpu>=1.8.0` - Base de datos vectorial
- `python-dotenv>=1.0.0` - Manejo de variables de entorno

---

## Configuraci√≥n

### 1. Crear Archivo `.env`

Crea un archivo `.env` en la ra√≠z del proyecto:

```bash
# Para usar Google Gemini (Recomendado)
GOOGLE_API_KEY=tu_clave_de_google_aqu√≠

# Para usar Hugging Face (Opcional)
HUGGINGFACEHUB_API_TOKEN=tu_token_de_huggingface_aqu√≠
```

### 2. Seleccionar Proveedor LLM

En `src/agent.py`, l√≠nea 41:

```python
LLM_PROVIDER = "GOOGLE"  # Opciones: "GOOGLE" o "HUGGINGFACE"
```

### 3. Personalizar Datos (Opcional)

#### **FAQs** (`data/faq.json`):
```json
[
  {
    "pregunta": "¬øCu√°l es el correo de soporte t√©cnico?",
    "respuesta": "El correo de soporte t√©cnico es soporte.ti@une.edu.py."
  }
]
```

#### **Reglamentos** (`data/reglamentos.txt`):
A√±ade el texto completo de tus reglamentos acad√©micos. El sistema crear√° autom√°ticamente los embeddings.

---

## Uso

### Modo Interactivo (CLI)

Ejecuta el asistente desde la terminal:

```bash
python -m src.main
```

**Ejemplo de conversaci√≥n:**

```
¬°Hola! Soy tu asistente acad√©mico. ¬øEn qu√© puedo ayudarte?
Escribe 'salir' para terminar la conversaci√≥n.

T√∫: ¬øCu√°l es el promedio de 8, 9 y 7?

--- Usando el proveedor de LLM: GOOGLE ---

> Entering new AgentExecutor chain...
Question: ¬øCu√°l es el promedio de 8, 9 y 7?
Thought: El usuario quiere calcular un promedio. Debo usar la herramienta 'calculadora_academica'.
Action: calculadora_academica
Action Input: 8, 9 y 7
Observation: La media de 8.0, 9.0 y 7.0 es: 8.0
Thought: Ya tengo la respuesta final.
Final Answer: El promedio de 8, 9 y 7 es 8.0.

> Finished chain.

Asistente: El promedio de 8, 9 y 7 es 8.0.
```

### Notebooks de Jupyter

Abre los notebooks para pruebas interactivas:

```bash
jupyter lab
```

Luego navega a:
- `notebooks/01_prueba_calculadora.ipynb` - Pruebas de la calculadora y agente b√°sico
- `notebooks/02_prueba_buscador.ipynb` - Pruebas de b√∫squeda en FAQ y reglamentos

---

## Estructura del Proyecto

```
TP final IA/
‚îÇ
‚îú‚îÄ‚îÄ src/                          # C√≥digo fuente principal
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                  # Configuraci√≥n del agente y selector LLM
‚îÇ   ‚îú‚îÄ‚îÄ main.py                   # Punto de entrada (CLI interactivo)
‚îÇ   ‚îî‚îÄ‚îÄ tools/                    # Herramientas del agente
‚îÇ       ‚îú‚îÄ‚îÄ calculadora.py        # Calculadora de promedios
‚îÇ       ‚îî‚îÄ‚îÄ buscador.py           # B√∫squeda en FAQ y reglamentos (RAG)
‚îÇ
‚îú‚îÄ‚îÄ data/                         # Base de conocimientos
‚îÇ   ‚îú‚îÄ‚îÄ faq.json                  # Preguntas frecuentes
‚îÇ   ‚îî‚îÄ‚îÄ reglamentos.txt           # Documento de reglamentos acad√©micos
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                    # Jupyter notebooks de demostraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ 01_prueba_calculadora.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 02_prueba_buscador.ipynb
‚îÇ
‚îú‚îÄ‚îÄ .env                          # Variables de entorno (API keys)
‚îú‚îÄ‚îÄ .gitignore                    # Archivos ignorados por git
‚îú‚îÄ‚îÄ requirements.txt              # Dependencias del proyecto
‚îî‚îÄ‚îÄ README.md                     # Este archivo
```

---

## Herramientas Disponibles

### 1. Calculadora Acad√©mica

**Archivo**: `src/tools/calculadora.py`

**Funci√≥n**: Extrae n√∫meros de texto y calcula promedios.

**Tecnolog√≠as**:
- Expresiones regulares para extracci√≥n de n√∫meros
- Soporte para formatos: `8`, `8.5`, `8,5`

**Ejemplo de uso**:
```python
from src.tools.calculadora import calcular_promedio_de_notas

resultado = calcular_promedio_de_notas("¬øCu√°l es el promedio de 7, 8 y 9?")
# Output: "La media de 7.0, 8.0 y 9.0 es: 8.0"
```

### 2. Buscador de FAQ

**Archivo**: `src/tools/buscador.py` ‚Üí funci√≥n `buscar_en_faq()`

**Funci√≥n**: Busca coincidencias en preguntas frecuentes.

**Tecnolog√≠as**:
- Carga de JSON con respuestas predefinidas
- B√∫squeda por similitud de texto simple

**Ejemplo de uso**:
```python
from src.tools.buscador import buscar_en_faq

respuesta = buscar_en_faq("¬øCu√°l es el correo de soporte?")
# Output: "El correo de soporte t√©cnico es soporte.ti@une.edu.py."
```

### 3. Buscador Sem√°ntico de Reglamentos (RAG)

**Archivo**: `src/tools/buscador.py` ‚Üí clase `BuscadorSemantico`

**Funci√≥n**: B√∫squeda sem√°ntica en documentos largos usando embeddings vectoriales.

**Tecnolog√≠as**:
- **Sentence-Transformers**: `paraphrase-multilingual-MiniLM-L12-v2`
- **FAISS**: Base de datos vectorial para b√∫squeda r√°pida
- **LangChain**: `RecursiveCharacterTextSplitter` para chunking

**Flujo**:
1. Divide el documento en chunks de 300 caracteres
2. Crea embeddings vectoriales para cada chunk
3. Almacena en FAISS para b√∫squeda eficiente
4. Recupera los 2 chunks m√°s relevantes para cada consulta

**Ejemplo de uso**:
```python
from src.tools.buscador import buscador_de_reglamentos

respuesta = buscador_de_reglamentos.buscar("¬øCu√°l es la nota m√≠nima para aprobar?")
# Output: "La nota m√≠nima para aprobar una asignatura es 6.0..."
```

---

## Selector de Proveedores LLM

### Arquitectura del Selector

El sistema permite cambiar f√°cilmente entre proveedores de modelos de lenguaje editando **una sola l√≠nea** en `src/agent.py`:

```python
LLM_PROVIDER = "GOOGLE"  # Cambia a "HUGGINGFACE" para usar Mistral
```

### Proveedores Disponibles

#### 1. **Google Gemini** (Por defecto)

**Ventajas**:
- ‚úÖ R√°pido y eficiente
- ‚úÖ API gratuita con l√≠mites generosos
- ‚úÖ Excelente comprensi√≥n del espa√±ol
- ‚úÖ Baja latencia

**Configuraci√≥n**:
```python
if LLM_PROVIDER == "GOOGLE":
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0.1  # Respuestas m√°s determin√≠sticas
    )
```

**Obtener API Key**:
1. Ve a [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Crea un nuevo proyecto
3. Genera una API key
4. A√±√°dela al `.env`: `GOOGLE_API_KEY=tu_clave`

#### 2. **Hugging Face (Mistral-7B)**

**Ventajas**:
- ‚úÖ Open source
- ‚úÖ Control total sobre el modelo
- ‚úÖ Sin l√≠mites de uso (dependiendo del plan)
- ‚úÖ Puede ejecutarse localmente

**Configuraci√≥n**:
```python
elif LLM_PROVIDER == "HUGGINGFACE":
    llm_endpoint = HuggingFaceEndpoint(
        repo_id="mistralai/Mistral-7B-Instruct-v0.2",
        task="text-generation",
        max_new_tokens=256,
        temperature=0.1,
        stop_sequences=["\nObservation:", "\nThought:"]
    )
    llm = ChatHuggingFace(llm=llm_endpoint)
```

**Obtener Token**:
1. Reg√≠strate en [Hugging Face](https://huggingface.co/)
2. Ve a [Settings ‚Üí Access Tokens](https://huggingface.co/settings/tokens)
3. Crea un token de tipo "Read"
4. A√±√°delo al `.env`: `HUGGINGFACEHUB_API_TOKEN=tu_token`

### A√±adir Nuevos Proveedores

Para a√±adir un nuevo proveedor (ejemplo: OpenAI):

1. Instala el cliente:
```bash
pip install langchain-openai
```

2. A√±ade el import en `src/agent.py`:
```python
from langchain_openai import ChatOpenAI
```

3. A√±ade la condici√≥n en el selector:
```python
elif LLM_PROVIDER == "OPENAI":
    llm = ChatOpenAI(
        model="gpt-4",
        temperature=0.1,
        api_key=os.getenv("OPENAI_API_KEY")
    )
```

---

## üìì Notebooks de Demostraci√≥n

### Notebook 1: Prueba de Calculadora

**Archivo**: `notebooks/01_prueba_calculadora.ipynb`

**Contenido**:
1. Importaci√≥n y configuraci√≥n del agente
2. Pruebas unitarias de la calculadora
3. Pruebas del agente completo con selector LLM

**Ejecutar**:
```bash
jupyter lab notebooks/01_prueba_calculadora.ipynb
```

### Notebook 2: Prueba de Buscadores

**Archivo**: `notebooks/02_prueba_buscador.ipynb`

**Contenido**:
1. Pruebas del buscador de FAQ
2. Pruebas del buscador sem√°ntico de reglamentos
3. Pruebas del agente completo con todas las herramientas

**Ejecutar**:
```bash
jupyter lab notebooks/02_prueba_buscador.ipynb
```

---

## Troubleshooting

### Error: "Module not found: langchain_google_genai"

**Soluci√≥n**:
```bash
pip install langchain-google-genai google-generativeai
```

### Error: "Invalid API key"

**Soluci√≥n**:
1. Verifica que el archivo `.env` est√© en la ra√≠z del proyecto
2. Aseg√∫rate de que la clave sea correcta y est√© activa
3. Reinicia el kernel de Jupyter si est√°s usando notebooks

### Agente entra en bucle infinito

**Soluci√≥n**:
El prompt tiene `max_iterations=5` para prevenir esto. Si ocurre:
1. Reduce `max_new_tokens` en el endpoint de Hugging Face
2. Aseg√∫rate de que `stop_sequences` est√© configurado correctamente
3. Cambia a Google Gemini, que es m√°s estable

### B√∫squeda sem√°ntica muy lenta

**Soluci√≥n**:
1. Usa `faiss-cpu` en lugar de `faiss-gpu` para desarrollo local
2. Reduce el tama√±o de los chunks en `buscador.py`:
```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,  # Reduce de 300 a 200
    chunk_overlap=30
)
```

---

## Contribuir

Las contribuciones son bienvenidas. Para contribuir:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/NuevaCaracteristica`)
3. Commit tus cambios (`git commit -m 'A√±adir nueva caracter√≠stica'`)
4. Push a la rama (`git push origin feature/NuevaCaracteristica`)
5. Abre un Pull Request

### Ideas para Contribuir

- A√±adir m√°s herramientas (clima, horarios de transporte, etc.)
- Mejorar el soporte multiidioma
- A√±adir dashboard con Streamlit
- Mejorar la interfaz CLI con `rich` o `typer`
- A√±adir tests unitarios con `pytest`
- Mejorar la documentaci√≥n

---

## Licencia

Este proyecto est√° bajo la licencia MIT.

---

## Autor

Desarrollado como proyecto final para la materia **Inteligencia Artificial I**.

---

## Agradecimientos

- **LangChain** - Framework de orquestaci√≥n de agentes
- **Google** - API de Gemini
- **Hugging Face** - Modelos open source
- **FAISS** - B√∫squeda vectorial eficiente
- **Sentence-Transformers** - Embeddings multiidioma

---

## Referencias y Recursos

- [Documentaci√≥n de LangChain](https://python.langchain.com/)
- [Google Gemini API Docs](https://ai.google.dev/)
- [Hugging Face Inference API](https://huggingface.co/docs/api-inference/)
- [FAISS Documentation](https://github.com/facebookresearch/faiss)
- [Sentence-Transformers](https://www.sbert.net/)

---

<div align="center">

**‚≠ê Si te gust√≥ el proyecto, dale una estrella en GitHub ‚≠ê**

Made with ‚ù§Ô∏è and ü§ñ

</div>
